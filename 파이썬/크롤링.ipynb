{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "146ea8d9-8009-4b69-8d91-4eb31e397271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.18\n"
     ]
    }
   ],
   "source": [
    "! python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0963bfe4-a65c-4a25-a6ff-f606a0886fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 100개 수집 완료\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv, json, time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE_URL = \"http://quotes.toscrape.com/\"\n",
    "\n",
    "def crawl():\n",
    "    url = BASE_URL\n",
    "    results = []\n",
    "    while url:\n",
    "        resp = requests.get(url)\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        \n",
    "        # 명언, 저자, 태그 추출\n",
    "        for q in soup.select(\"div.quote\"):\n",
    "            text = q.select_one(\"span.text\").get_text(strip=True).strip(\"“”\")\n",
    "            author = q.select_one(\"small.author\").get_text(strip=True)\n",
    "            tags = [t.get_text(strip=True) for t in q.select(\"a.tag\")]\n",
    "            results.append({\"quote\": text, \"author\": author, \"tags\": tags})\n",
    "        \n",
    "        # 다음 페이지 확인\n",
    "        next_link = soup.select_one(\"li.next a\")\n",
    "        url = urljoin(url, next_link[\"href\"]) if next_link else None\n",
    "        time.sleep(0.5)  # 예의상 대기\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = crawl()\n",
    "    print(f\"총 {len(data)}개 수집 완료\")\n",
    "\n",
    "    # CSV 저장\n",
    "    with open(\"quotes.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"quote\", \"author\", \"tags\"])\n",
    "        for row in data:\n",
    "            writer.writerow([row[\"quote\"], row[\"author\"], \"|\".join(row[\"tags\"])])\n",
    "\n",
    "    # JSONL 저장\n",
    "    with open(\"quotes.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in data:\n",
    "            json.dump(row, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e67ef1-b148-4629-ae88-57e73bdf6b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일 저장 완료: books.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# 사이트 URL\n",
    "url = 'https://books.toscrape.com/'\n",
    "\n",
    "# 요청 보내기\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # 오류 발생 시 예외\n",
    "\n",
    "# HTML 파싱\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# 책 리스트 가져오기\n",
    "books = soup.select('section > div:nth-child(2) > ol > li')\n",
    "\n",
    "# 별점과 제목 추출\n",
    "book_data = []\n",
    "for book in books:\n",
    "    title_tag = book.select_one('h3 > a')\n",
    "    rating_tag = book.select_one('p.star-rating')\n",
    "    \n",
    "    if title_tag and rating_tag:\n",
    "        title = title_tag['title']  # title 속성에 책 제목이 있음\n",
    "        # 별점은 클래스에서 추출 (예: 'star-rating Three')\n",
    "        rating_class = rating_tag.get('class', [])\n",
    "        rating = rating_class[1] if len(rating_class) > 1 else 'None'\n",
    "        \n",
    "        book_data.append([title, rating])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "with open('books.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Title', 'Rating'])  # 헤더\n",
    "    writer.writerows(book_data)\n",
    "\n",
    "print('CSV 파일 저장 완료: books.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ec2e40-d94b-4ba9-a1a1-e6a5848ea95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (4.35.0)\n",
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from selenium) (2025.8.3)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from selenium) (4.14.1)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from trio~=0.30.0->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from trio~=0.30.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from trio~=0.30.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from trio~=0.30.0->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from webdriver-manager) (2.32.4)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/kdt/lib/python3.10/site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [webdriver-manager]\n",
      "\u001b[1A\u001b[2KSuccessfully installed python-dotenv-1.1.1 webdriver-manager-4.0.2\n"
     ]
    }
   ],
   "source": [
    "! pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6540182e-b14c-4c5d-8931-32a605108ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] chart/top 수집 시작: https://www.imdb.com/chart/top/\n",
      "[INFO] JSON-LD 250개\n",
      "  1. The Shawshank Redemption (None) ★9.3\n",
      "  2. The Godfather (None) ★9.2\n",
      "  3. The Dark Knight (None) ★9.1\n",
      "  4. The Godfather Part II (None) ★9.0\n",
      "  5. 12 Angry Men (None) ★9.0\n",
      "  6. The Lord of the Rings: The Return of the King (None) ★9.0\n",
      "  7. Schindler&apos;s List (None) ★9.0\n",
      "  8. Pulp Fiction (None) ★8.8\n",
      "  9. The Lord of the Rings: The Fellowship of the Ring (None) ★8.9\n",
      " 10. Il buono, il brutto, il cattivo (None) ★8.8\n",
      "[INFO] 저장 완료 -> CSV: imdb_top250.csv, JSONL: imdb_top250.jsonl\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "IMDb Top 250 (chart/top) 수집 스크립트\n",
    "- 대상: https://www.imdb.com/chart/top/\n",
    "- 1순위: JSON-LD(ItemList) 파싱\n",
    "- 2순위: 테이블 마크업 파싱(폴백)\n",
    "- 출력: imdb_top250.csv, imdb_top250.jsonl\n",
    "\"\"\"\n",
    "\n",
    "import time, re, sys, csv, json\n",
    "from typing import List, Dict, Optional\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://www.imdb.com/chart/top/\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (training-bot; +https://example.com) Requests/2.x\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9,ko;q=0.8\",\n",
    "}\n",
    "SLEEP = 0.5\n",
    "TIMEOUT = 15\n",
    "RETRIES = 3\n",
    "BACKOFF = 1.7\n",
    "\n",
    "OUT_CSV = \"imdb_top250.csv\"\n",
    "OUT_JSONL = \"imdb_top250.jsonl\"\n",
    "\n",
    "def fetch_html(url: str) -> str:\n",
    "    last = None\n",
    "    for i in range(1, RETRIES + 1):\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "            r.raise_for_status()\n",
    "            return r.text\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "            time.sleep(BACKOFF ** (i - 1))\n",
    "    raise RuntimeError(f\"Fetch failed: {url} ({last})\")\n",
    "\n",
    "def _to_int(s: Optional[str]) -> Optional[int]:\n",
    "    if not s: return None\n",
    "    m = re.search(r\"\\b(19|20)\\d{2}\\b\", s) or re.search(r\"\\d+\", s)\n",
    "    return int(m.group(0)) if m else None\n",
    "\n",
    "def _to_float(s: Optional[str]) -> Optional[float]:\n",
    "    if not s: return None\n",
    "    m = re.search(r\"\\d+(?:\\.\\d+)?\", s)\n",
    "    return float(m.group(0)) if m else None\n",
    "\n",
    "# 1) JSON-LD 우선 파싱\n",
    "def parse_jsonld(html: str, base_url: str) -> List[Dict]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    items: List[Dict] = []\n",
    "    for tag in soup.select('script[type=\"application/ld+json\"]'):\n",
    "        txt = tag.string or \"\"\n",
    "        if not txt.strip():\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(txt)\n",
    "        except Exception:\n",
    "            continue\n",
    "        blocks = data if isinstance(data, list) else [data]\n",
    "        for b in blocks:\n",
    "            if not isinstance(b, dict):\n",
    "                continue\n",
    "            # ItemList 혹은 CollectionPage 내부의 itemListElement\n",
    "            if b.get(\"@type\") in (\"ItemList\", \"CollectionPage\"):\n",
    "                elements = b.get(\"itemListElement\") or []\n",
    "                for pos, el in enumerate(elements, start=1):\n",
    "                    node = el.get(\"item\") if isinstance(el, dict) else el\n",
    "                    if not isinstance(node, dict):\n",
    "                        continue\n",
    "                    title = node.get(\"name\") or \"\"\n",
    "                    url = node.get(\"url\") or \"\"\n",
    "                    agg = node.get(\"aggregateRating\") or {}\n",
    "                    rating = agg.get(\"ratingValue\")\n",
    "                    year = _to_int(node.get(\"datePublished\") or node.get(\"copyrightYear\"))\n",
    "                    if not url.startswith(\"http\"):\n",
    "                        url = urljoin(base_url, url)\n",
    "                    if title:\n",
    "                        items.append({\n",
    "                            \"rank\": pos,\n",
    "                            \"title\": title,\n",
    "                            \"year\": year,\n",
    "                            \"rating\": float(rating) if rating else None,\n",
    "                            \"title_url\": url,\n",
    "                            \"source_page\": base_url\n",
    "                        })\n",
    "    return items\n",
    "\n",
    "# 2) 테이블 폴백 파싱\n",
    "def parse_table(html: str, page_url: str) -> List[Dict]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    rows = soup.select(\"table tbody tr\") or soup.select(\"tbody tr\")\n",
    "    out: List[Dict] = []\n",
    "    for i, tr in enumerate(rows, start=1):\n",
    "        a = tr.select_one(\".titleColumn a, a[href*='/title/']\")\n",
    "        title = a.get_text(strip=True) if a else \"\"\n",
    "        href = a.get(\"href\") if a else \"\"\n",
    "        url = urljoin(page_url, href) if href else \"\"\n",
    "        year_el = tr.select_one(\".titleColumn span, span.secondaryInfo\")\n",
    "        year = _to_int(year_el.get_text(strip=True) if year_el else \"\")\n",
    "        rating_el = tr.select_one(\".imdbRating strong, td.ratingColumn strong, strong\")\n",
    "        rating = _to_float(rating_el.get_text(strip=True) if rating_el else \"\")\n",
    "        if title:\n",
    "            out.append({\n",
    "                \"rank\": i,\n",
    "                \"title\": title,\n",
    "                \"year\": year,\n",
    "                \"rating\": rating,\n",
    "                \"title_url\": url,\n",
    "                \"source_page\": page_url\n",
    "            })\n",
    "    return out\n",
    "\n",
    "def save_csv(rows: List[Dict], path: str = OUT_CSV) -> None:\n",
    "    fields = [\"rank\", \"title\", \"year\", \"rating\", \"title_url\", \"source_page\"]\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(fields)\n",
    "        for r in rows:\n",
    "            w.writerow([r.get(k, \"\") for k in fields])\n",
    "\n",
    "def save_jsonl(rows: List[Dict], path: str = OUT_JSONL) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            json.dump(r, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "def main():\n",
    "    print(\"[INFO] chart/top 수집 시작:\", URL)\n",
    "    html = fetch_html(URL)\n",
    "\n",
    "    # 1) JSON-LD 먼저\n",
    "    data = parse_jsonld(html, URL)\n",
    "    if data:\n",
    "        print(f\"[INFO] JSON-LD {len(data)}개\")\n",
    "    else:\n",
    "        # 2) 폴백: 테이블 파싱\n",
    "        data = parse_table(html, URL)\n",
    "        print(f\"[INFO] table {len(data)}개\")\n",
    "\n",
    "    if not data:\n",
    "        raise RuntimeError(\"아이템을 찾지 못했습니다. 선택자를 다시 확인하세요.\")\n",
    "\n",
    "    # 샘플 출력\n",
    "    for r in data[:10]:\n",
    "        print(f\"{r['rank']:>3}. {r['title']} ({r.get('year')}) ★{r.get('rating')}\")\n",
    "\n",
    "    save_csv(data, OUT_CSV)\n",
    "    save_jsonl(data, OUT_JSONL)\n",
    "    print(f\"[INFO] 저장 완료 -> CSV: {OUT_CSV}, JSONL: {OUT_JSONL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR]\", e, file=sys.stderr)\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f39e5f-3a22-40e4-8431-251e3e741842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd5193-b779-495c-8209-65beb6b6e705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5c39e-5a6d-404c-9c31-4e3e541e530c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1e8ec-9831-4333-bd3d-74e19d37b660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
